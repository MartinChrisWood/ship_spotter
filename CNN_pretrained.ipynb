{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things I want to learn/try:\n",
    "- Data flow from files (for bigger data/projects)\n",
    "- Loading and tuning a pre-trained model\n",
    "- Layer visualisation methods\n",
    "- Picking out the miss-classified from validation, and displaying them\n",
    "- Non-sequential models (No excuse yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The very first time you import keras and seaborn, there's a long delay as\n",
    "# setup stuff happens\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from keras import layers, models, optimizers\n",
    "from keras.applications import VGG16\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# This contains a few useful functions for reshaping, plotting etc\n",
    "import src.helpers as h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/shipsnet.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Data structure\n",
    "print([key for key in data.keys()])\n",
    "\n",
    "# Check labels\n",
    "print(data[\"labels\"][:10], data[\"labels\"][-10:])\n",
    "\n",
    "# Check labels split\n",
    "print(\"True: \", sum([i == 1 for i in data[\"labels\"]]))\n",
    "print(\"False: \", sum([i == 0 for i in data[\"labels\"]]))\n",
    "\n",
    "# Plot an example to check\n",
    "h.quick_plot_img(data[\"data\"][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data - in this case I let validation split be handled by the\n",
    "# training data generator below, rather than doing it manually\n",
    "train_X, train_y, val_X, val_y, test_X, test_y = h.train_test_validation_split(\n",
    "    data[\"data\"], data[\"labels\"]\n",
    ")\n",
    "\n",
    "# Reformat the split features into (N, 80, 80, 3) shape array\n",
    "train_X = h.format_imgs(train_X)\n",
    "val_X = h.format_imgs(val_X)\n",
    "test_X = h.format_imgs(test_X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Make data generators (implements data augmentation steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure data generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode=\"nearest\",\n",
    ")\n",
    "\n",
    "# For test, obviously no augmentation\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "# Feed the generators the source data\n",
    "# DEPRECATION since book: class_mode now auto-detected? Unsure.\n",
    "# NOTE: Shuffling and creation of a validation set can be handled by the\n",
    "# generator, by passing the flow method 'shuffle=True' and by passing model.fit\n",
    "# 'subset = \"training\"' or 'subset=\"validation\"' respectively\n",
    "train_generator = train_datagen.flow(\n",
    "    train_X, train_y, batch_size=20, shuffle=False\n",
    ")  # noqa:E501\n",
    "\n",
    "validation_generator = train_datagen.flow(\n",
    "    val_X, val_y, batch_size=20, shuffle=False\n",
    ")  # noqa:E501\n",
    "\n",
    "test_generator = test_datagen.flow(\n",
    "    test_X, test_y, batch_size=20, shuffle=False\n",
    ")  # noqa:E501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.quick_plot_imggen(train_X[1], train_datagen)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "# Set up the pre-trained vision NN\n",
    "conv_base = VGG16(\n",
    "    weights=\"imagenet\",  # Which trained model of a set\n",
    "    include_top=False,  # Don't include classification layers\n",
    "    input_shape=(80, 80, 3),\n",
    ")  # Set input shape to own data\n",
    "\n",
    "# Let's make sure we don't try to train these 14 mil parameters\n",
    "conv_base.trainable = False\n",
    "\n",
    "# These are the deep layers that develop into feature extractors\n",
    "model.add(conv_base)\n",
    "\n",
    "# And these are essentially a less complex classifier sat on top\n",
    "# Note use of Dropout\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1024, activation=\"relu\"))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Not sure what specifically is computed at compilation, I guess this is where\n",
    "# the backprop formulae etc are determined?\n",
    "# DEPRECATION since book:  arg 'lr' replaced with 'learning_rate' for optimizer\n",
    "# On a very small model 'steps_per_execution' can be raised,\n",
    "# reducing python overhead\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=optimizers.RMSprop(learning_rate=1e-4),\n",
    "    metrics=[\"acc\"],\n",
    ")\n",
    "\n",
    "# View a summary - useful for improvising a build\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop#\n",
    "# DEPRECATION since book:  method 'fit_generator', now just use 'fit'\n",
    "# steps_per_epoch doesn't need to be specified if you've specified batch size\n",
    "# in a generator based on a loaded dataset, but might be needed if you're\n",
    "# streaming data from a directory.\n",
    "# Likewise for validation_steps\n",
    "history = model.fit(\n",
    "    train_generator, epochs=50, validation_data=validation_generator\n",
    ")  # noqa:E501\n",
    "\n",
    "model.save(\"models/ship_spotting_v0.1.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Review Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm going to use pandas and seaborn if I can for results plots\n",
    "results = pd.DataFrame(history.history)\n",
    "\n",
    "# Add 1 to the index values, so they go 1-30 rather than 0-29 (pure aesthetics)\n",
    "results.index = results.index + 1\n",
    "\n",
    "# Quick look at final performance\n",
    "print(results.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=results[[\"acc\", \"val_acc\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=results[[\"loss\", \"val_loss\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = model.evaluate(test_generator)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "9fcc4401f6cbc57dcd7807fa5ff1a0d07284a075e8329a5d7d45368ab61a99cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
