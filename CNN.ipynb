{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things I want to learn/try:\n",
    "- Data flow from files (for bigger data/projects)\n",
    "- Loading and tuning a pre-trained model\n",
    "- Layer visualisation methods\n",
    "- Picking out the miss-classified from validation, and displaying them\n",
    "- Non-sequential models (No excuse yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The very first time you import keras and seaborn, there's a long delay as\n",
    "# setup stuff happens\n",
    "import json\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from keras import layers, models, optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# This contains a few useful functions for reshaping, plotting etc\n",
    "import src.helpers as h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/shipsnet.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Data structure\n",
    "print([key for key in data.keys()])\n",
    "\n",
    "# Check labels\n",
    "print(data[\"labels\"][:10], data[\"labels\"][-10:])\n",
    "\n",
    "# Check labels split\n",
    "print(\"True: \", sum([i == 1 for i in data[\"labels\"]]))\n",
    "print(\"False: \", sum([i == 0 for i in data[\"labels\"]]))\n",
    "\n",
    "# Plot an example to check\n",
    "h.quick_plot_img(data[\"data\"][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data - in this case just train and test\n",
    "train_X, train_y, val_X, val_y, test_X, test_y = h.train_test_validation_split(\n",
    "    data[\"data\"], data[\"labels\"], validation=True\n",
    ")\n",
    "\n",
    "# Reformat the split features into (N, 80, 80, 3) shape array\n",
    "train_X = h.format_imgs(train_X)\n",
    "val_X = h.format_imgs(val_X)\n",
    "test_X = h.format_imgs(test_X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Make data generators (implements data augmentation steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure data generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode=\"nearest\",\n",
    ")\n",
    "\n",
    "# For test, obviously no augmentation\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "# Feed the generators the source data\n",
    "# DEPRECATION since book: class_mode now auto-detected? Unsure.\n",
    "# NOTE: Shuffling and creation of a validation set can be handled by the\n",
    "# generator, by passing the flow method 'shuffle=True' and by passing model.fit\n",
    "# 'subset = \"training\"' or 'subset=\"validation\"' respectively\n",
    "train_generator = train_datagen.flow(\n",
    "    train_X, train_y, batch_size=20, shuffle=False\n",
    ")  # noqa:E501\n",
    "validation_generator = test_datagen.flow(\n",
    "    val_X, val_y, batch_size=20, shuffle=False\n",
    ")  # noqa:E501\n",
    "test_generator = test_datagen.flow(\n",
    "    test_X, test_y, batch_size=20, shuffle=False\n",
    ")  # noqa:E501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.quick_plot_imggen(train_X[1], train_datagen)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "# These are the deep layers that develop into feature extractors\n",
    "model.add(\n",
    "    layers.Conv2D(64, (3, 3), activation=\"relu\", input_shape=(80, 80, 3))\n",
    ")  # noqa:E501\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(256, (3, 3), activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(512, (3, 3), activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# And these are essentially a less complex classifier sat on top\n",
    "# Note use of Dropout\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1024, activation=\"relu\"))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Not sure what specifically is computed at compilation, I guess this is where\n",
    "# the backprop formulae etc are determined?\n",
    "# DEPRECATION since book:  arg 'lr' replaced with 'learning_rate' for optimizer\n",
    "# On a very small model 'steps_per_execution' can be raised,\n",
    "# reducing python overhead\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=optimizers.RMSprop(learning_rate=1e-4),\n",
    "    metrics=[\"acc\"],\n",
    ")\n",
    "\n",
    "# View a summary - useful for improvising a build\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop#\n",
    "# DEPRECATION since book:  method 'fit_generator', now just use 'fit'\n",
    "# steps_per_epoch doesn't need to be specified if you've specified batch size\n",
    "# in a generator based on a loaded dataset, but might be needed if you're\n",
    "# streaming data from a directory.\n",
    "# Likewise for validation_steps\n",
    "history = model.fit(\n",
    "    train_generator, epochs=50, validation_data=validation_generator\n",
    ")  # noqa:E501\n",
    "\n",
    "model.save(\"models/ship_spotting_v0.1.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Review Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm going to use pandas and seaborn if I can for results plots\n",
    "results = pd.DataFrame(history.history)\n",
    "\n",
    "# Add 1 to the index values, so they go 1-30 rather than 0-29 (pure aesthetics)\n",
    "results.index = results.index + 1\n",
    "\n",
    "# Get rolling averages for a smoother look\n",
    "for col in results.columns:\n",
    "    results[col + \"_rolling\"] = results[col].rolling(3, center=True).mean()\n",
    "\n",
    "print(results.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=results[[\"acc_rolling\", \"val_acc_rolling\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=results[[\"loss_rolling\", \"val_loss_rolling\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_generator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Review (validation) predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify records in validation that are miss-identified\n",
    "prob = model.predict(validation_generator).flatten()\n",
    "pred = np.round(prob)\n",
    "\n",
    "fails = []\n",
    "for i in range(len(pred)):\n",
    "    if pred[i] != val_y[i]:\n",
    "        fails.append(i)\n",
    "\n",
    "for example in fails:\n",
    "    img = np.reshape(val_X[example], (80, 80, 3), order=\"F\")\n",
    "    plt.clf()\n",
    "    plt.imshow((img))\n",
    "    plt.text(\n",
    "        2,\n",
    "        5,\n",
    "        f\"True: {val_y[example]}, Pred: {int(pred[example])}\",\n",
    "        fontsize=15,  # noqa:E501\n",
    "    )\n",
    "    plt.savefig(os.path.join(\"outputs\", \"fails\", f\"validation_{example}.png\"))\n",
    "\n",
    "print(\"Fails output to file for examination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "h.quick_plot_img(val_X[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [layer.output for layer in model.layers[:8]]\n",
    "activation_model = models.Model(inputs=model.input, outputs=layer_outputs)\n",
    "activations = activation_model.predict(val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer_activation = activations[0]\n",
    "print(first_layer_activation.shape)\n",
    "plt.matshow(first_layer_activation[index, :, :, 16], cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_activation(model, features):\n",
    "\n",
    "    # Fetch raw layer output for each sample for every layer\n",
    "    layer_outputs = [layer.output for layer in model.layers]\n",
    "    activation_model = models.Model(inputs=model.input, outputs=layer_outputs)\n",
    "    activations = activation_model.predict(features)\n",
    "\n",
    "    layer_names = []\n",
    "    for layer in model.layers:\n",
    "        layer_names.append(layer.name)\n",
    "\n",
    "    images_per_row = 16\n",
    "\n",
    "    # Visualise each layer\n",
    "    for layer_name, layer_activation in zip(layer_names, activations):\n",
    "        if \"2d\" in layer_name:\n",
    "            n_features = layer_activation.shape[-1]\n",
    "            size = layer_activation.shape[1]\n",
    "            n_cols = n_features // images_per_row\n",
    "            display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "\n",
    "            for col in range(n_cols):\n",
    "                for row in range(images_per_row):\n",
    "                    channel_image = layer_activation[\n",
    "                        0, :, :, col * images_per_row + row\n",
    "                    ]\n",
    "\n",
    "                    channel_image -= channel_image.mean()\n",
    "                    channel_image /= channel_image.std()\n",
    "                    channel_image *= 64\n",
    "                    channel_image += 128\n",
    "                    channel_image = np.clip(channel_image, 0, 255).astype(\n",
    "                        \"uint8\"\n",
    "                    )  # noqa:E501\n",
    "                    display_grid[\n",
    "                        col * size : (col + 1) * size,  # noqa:E203\n",
    "                        row * size : (row + 1) * size,  # noqa:E203\n",
    "                    ] = channel_image\n",
    "\n",
    "            scale = 1.0 / size\n",
    "            plt.figure(\n",
    "                figsize=(\n",
    "                    scale * display_grid.shape[1],\n",
    "                    scale * display_grid.shape[0],\n",
    "                )  # noqa:E501\n",
    "            )\n",
    "            plt.title(layer_name)\n",
    "            plt.grid(False)\n",
    "            plt.imshow(display_grid, aspect=\"auto\", cmap=\"viridis\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "visualise_activation(model, val_X)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "9fcc4401f6cbc57dcd7807fa5ff1a0d07284a075e8329a5d7d45368ab61a99cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
